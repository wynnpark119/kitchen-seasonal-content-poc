"""
Database query helpers for Streamlit dashboard

ì½ê¸° ì „ìš© (SELECT only)
SQLAlchemy engineì„ ì‚¬ìš©í•˜ì—¬ ì»¤ë„¥ì…˜ í’€ ì¬ì‚¬ìš©
"""
import pandas as pd
from typing import List, Dict, Any, Optional
import streamlit as st
from sqlalchemy import text
import psycopg2

# SQLAlchemy engine ì‚¬ìš© (ì»¤ë„¥ì…˜ í’€ í¬í•¨)
from common.db import engine
from common.config import DATABASE_URL

def get_db_connection():
    """
    Get database connection from SQLAlchemy engine (ì»¤ë„¥ì…˜ í’€ ì¬ì‚¬ìš©)
    psycopg2 cursor í˜¸í™˜ì„ ìœ„í•´ raw connection ë°˜í™˜
    
    Returns:
        psycopg2 connection ë˜ëŠ” None
    """
    if not DATABASE_URL:
        return None
    try:
        # SQLAlchemy engineì—ì„œ raw psycopg2 connection ê°€ì ¸ì˜¤ê¸°
        raw_conn = engine.raw_connection()
        return raw_conn
    except Exception as e:
        print(f"Database connection error: {e}")
        return None

def query_to_dataframe(query: str, params: tuple = None) -> pd.DataFrame:
    """
    Execute query and return as DataFrame
    SQLAlchemy engineì„ ì‚¬ìš©í•˜ì—¬ ì»¤ë„¥ì…˜ í’€ ì¬ì‚¬ìš©
    """
    conn = get_db_connection()
    if conn is None:
        return pd.DataFrame()  # ë¹ˆ DataFrame ë°˜í™˜
    try:
        # psycopg2 cursor ì‚¬ìš© (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
        df = pd.read_sql_query(query, conn, params=params)
        return df
    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        print(f"Error executing query: {e}")
        print(f"Query: {query}")
        return pd.DataFrame()  # ë¹ˆ DataFrame ë°˜í™˜
    finally:
        if conn:
            conn.close()

def get_executive_overview() -> Dict[str, Any]:
    """Executive Overview ë°ì´í„° ì¡°íšŒ"""
    conn = get_db_connection()
    if conn is None:
        # DB ì—°ê²° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "total_topics": 0,
            "seasonal_count": 0,
            "evergreen_count": 0,
            "aio_available": 0,
            "aio_not_available": 0,
            "aio_error": 0,
            "lg_cited_count": 0,
            "top_topics": []
        }
    try:
        with conn.cursor() as cur:
            # ì „ì²´ Master Topic ìˆ˜ (topic_qa_briefsê°€ ì—†ìœ¼ë©´ clustersì—ì„œ ì¹´ìš´íŠ¸)
            try:
                cur.execute("""
                    SELECT COUNT(*) as total_topics
                    FROM topic_qa_briefs
                """)
                total_topics = cur.fetchone()[0] or 0
            except Exception as e:
                print(f"Error counting topic_qa_briefs: {e}")
                # topic_qa_briefsê°€ ì—†ìœ¼ë©´ clustersì—ì„œ ì¹´ìš´íŠ¸
                try:
                    cur.execute("""
                        SELECT COUNT(*) as total_topics
                        FROM clusters
                        WHERE noise_label = FALSE
                    """)
                    total_topics = cur.fetchone()[0] or 0
                except Exception as e2:
                    print(f"Error counting clusters: {e2}")
                    total_topics = 0
            
            # ì‹œì¦Œì„± vs ë¹„ì‹œì¦Œì„± ë¹„ìœ¨
            try:
                cur.execute("""
                    SELECT 
                        category,
                        COUNT(*) as count
                    FROM topic_qa_briefs
                    WHERE category IN ('SPRING_RECIPES', 'SPRING_KITCHEN_STYLING', 
                                        'REFRIGERATOR_ORGANIZATION', 'VEGETABLE_PREP_HANDLING')
                    GROUP BY category
                """)
                category_counts = {row[0]: row[1] for row in cur.fetchall()}
            except Exception as e:
                print(f"Error counting categories from topic_qa_briefs: {e}")
                # topic_qa_briefsê°€ ì—†ìœ¼ë©´ clustersì—ì„œ ì¹´ìš´íŠ¸
                try:
                    cur.execute("""
                        SELECT 
                            topic_category,
                            COUNT(*) as count
                        FROM clusters
                        WHERE noise_label = FALSE
                        AND topic_category IN ('SPRING_RECIPES', 'SPRING_KITCHEN_STYLING', 
                                               'REFRIGERATOR_ORGANIZATION', 'VEGETABLE_PREP_HANDLING')
                        GROUP BY topic_category
                    """)
                    category_counts = {row[0]: row[1] for row in cur.fetchall()}
                except Exception as e2:
                    print(f"Error counting categories from clusters: {e2}")
                    category_counts = {}
            
            seasonal_count = sum(category_counts.get(cat, 0) for cat in ['SPRING_RECIPES', 'SPRING_KITCHEN_STYLING'])
            evergreen_count = sum(category_counts.get(cat, 0) for cat in ['REFRIGERATOR_ORGANIZATION', 'VEGETABLE_PREP_HANDLING'])
            
            # AIO AVAILABLE vs NOT_AVAILABLE ë¹„ìœ¨ (aio_status ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ ìˆìŒ)
            aio_counts = {}
            try:
                cur.execute("""
                    SELECT 
                        COALESCE(aio_status, 'UNKNOWN') as aio_status,
                        COUNT(*) as count
                    FROM raw_serp_aio
                    GROUP BY COALESCE(aio_status, 'UNKNOWN')
                """)
                aio_counts = {row[0]: row[1] for row in cur.fetchall()}
            except Exception as e:
                print(f"Error counting aio_status: {e}")
                # aio_status ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ ì¹´ìš´íŠ¸ë§Œ
                try:
                    cur.execute("""
                        SELECT COUNT(*) as total
                        FROM raw_serp_aio
                    """)
                    total_serp = cur.fetchone()[0] or 0
                    if total_serp > 0:
                        aio_counts = {'UNKNOWN': total_serp}
                except Exception as e2:
                    print(f"Error counting raw_serp_aio: {e2}")
                    aio_counts = {}
            
            # LG ë„ë©”ì¸ ì¸ìš©ëœ Topic ìˆ˜
            lg_cited_count = 0
            try:
                cur.execute("""
                    SELECT COUNT(DISTINCT tqb.cluster_id) as lg_cited_count
                    FROM topic_qa_briefs tqb
                    JOIN raw_serp_aio sa ON sa.query ILIKE '%' || LOWER(tqb.topic_title) || '%'
                    WHERE COALESCE(sa.aio_status, '') = 'AVAILABLE'
                    AND (
                        sa.cited_sources_json::text ILIKE '%lge.com%'
                        OR sa.cited_sources_json::text ILIKE '%lg.com%'
                        OR sa.cited_sources_json::text ILIKE '%lgstory.com%'
                    )
                """)
                lg_cited_count = cur.fetchone()[0] or 0
            except Exception as e:
                print(f"Error counting LG cited topics: {e}")
                lg_cited_count = 0
            
            # ìµœê·¼ 3ê°œì›” ê¸°ì¤€ ìš°ì„  ê²€í†  Master Topic Top 5
            top_topics = []
            try:
                cur.execute("""
                    SELECT 
                        tqb.cluster_id,
                        tqb.topic_title,
                        tqb.category,
                        tqb.score,
                        tqb.insights_json->'evidence_strength'->>'score' as evidence_score
                    FROM topic_qa_briefs tqb
                    JOIN clusters c ON tqb.cluster_id = c.cluster_id
                    WHERE tqb.score IS NOT NULL
                    ORDER BY 
                        COALESCE((tqb.insights_json->'evidence_strength'->>'score')::int, 0) DESC,
                        tqb.score DESC
                    LIMIT 5
                """)
                top_topics = cur.fetchall()
            except Exception as e:
                print(f"Error fetching top topics: {e}")
                top_topics = []
            
            return {
                "total_topics": total_topics,
                "seasonal_count": seasonal_count,
                "evergreen_count": evergreen_count,
                "aio_available": aio_counts.get('AVAILABLE', 0),
                "aio_not_available": aio_counts.get('NOT_AVAILABLE', 0),
                "aio_error": aio_counts.get('ERROR', 0),
                "lg_cited_count": lg_cited_count,
                "top_topics": top_topics
            }
    except Exception as e:
        print(f"Error in get_executive_overview: {e}")
        import traceback
        traceback.print_exc()
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "total_topics": 0,
            "seasonal_count": 0,
            "evergreen_count": 0,
            "aio_available": 0,
            "aio_not_available": 0,
            "aio_error": 0,
            "lg_cited_count": 0,
            "top_topics": []
        }
    finally:
        if conn:
            conn.close()

def get_reddit_posts(keyword_filter: Optional[str] = None, limit: int = 1000) -> pd.DataFrame:
    """Reddit í¬ìŠ¤íŠ¸ ì¡°íšŒ"""
    conn = get_db_connection()
    if conn is None:
        return pd.DataFrame()  # ë¹ˆ DataFrame ë°˜í™˜
    
    try:
        query = """
            SELECT 
                reddit_post_id,
                keyword,
                title,
                upvotes,
                num_comments,
                TO_TIMESTAMP(created_utc) as created_at,
                permalink,
                subreddit
            FROM raw_reddit_posts
        """
        params = None
        if keyword_filter:
            query += " WHERE keyword ILIKE %s"
            params = (f"%{keyword_filter}%",)
        
        query += " ORDER BY upvotes DESC, num_comments DESC LIMIT %s"
        if params:
            params = params + (limit,)
        else:
            params = (limit,)
        
        df = pd.read_sql_query(query, conn, params=params)
        return df
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ DataFrame ë°˜í™˜ (í˜¸ì¶œìê°€ ì²˜ë¦¬)
        print(f"Error in get_reddit_posts: {e}")
        return pd.DataFrame()
    finally:
        if conn:
            conn.close()

def get_serp_aio() -> pd.DataFrame:
    """SERP AI Overview ì¡°íšŒ (raw_serp_aio + serp_results í†µí•©)"""
    conn = get_db_connection()
    if conn is None:
        print("Error: Database connection failed in get_serp_aio")
        return pd.DataFrame()
    
    try:
        dfs = []
        
        # 1. raw_serp_aio í…Œì´ë¸” ì¡°íšŒ
        with conn.cursor() as cur:
            # raw_serp_aio í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cur.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'raw_serp_aio'
                )
            """)
            raw_serp_aio_exists = cur.fetchone()[0]
            
            if raw_serp_aio_exists:
                cur.execute("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = 'raw_serp_aio'
                """)
                columns = [row[0] for row in cur.fetchall()]
                has_aio_status = 'aio_status' in columns
                
                if has_aio_status:
                    query_aio = """
                        SELECT 
                            query,
                            COALESCE(aio_status, 'UNKNOWN') as aio_status,
                            aio_text,
                            cited_sources_json,
                            snapshot_at,
                            COALESCE(locale, 'en-US') as locale,
                            'raw_serp_aio' as source_table
                        FROM raw_serp_aio
                        ORDER BY snapshot_at DESC
                    """
                else:
                    query_aio = """
                        SELECT 
                            query,
                            'UNKNOWN' as aio_status,
                            aio_text,
                            cited_sources_json,
                            snapshot_at,
                            COALESCE(locale, 'en-US') as locale,
                            'raw_serp_aio' as source_table
                        FROM raw_serp_aio
                        ORDER BY snapshot_at DESC
                    """
                
                cur.execute("SELECT COUNT(*) FROM raw_serp_aio")
                count_aio = cur.fetchone()[0]
                print(f"raw_serp_aio í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜: {count_aio}")
                
                if count_aio > 0:
                    df_aio = pd.read_sql_query(query_aio, conn)
                    dfs.append(df_aio)
                    print(f"raw_serp_aioì—ì„œ {len(df_aio)}ê°œ ë ˆì½”ë“œ ì¡°íšŒ")
            
            # 2. serp_results í…Œì´ë¸” ì¡°íšŒ (ì¿¼ë¦¬ë³„ë¡œ ê·¸ë£¹í™”)
            cur.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'serp_results'
                )
            """)
            serp_results_exists = cur.fetchone()[0]
            
            if serp_results_exists:
                # serp_resultsì—ì„œ ì¿¼ë¦¬ë³„ë¡œ ì§‘ê³„í•˜ì—¬ raw_serp_aio í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                # cited_sources_jsonì€ parse_cited_sources í•¨ìˆ˜ê°€ ê¸°ëŒ€í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ìƒì„±
                query_serp = """
                    SELECT 
                        sr.query,
                        'AVAILABLE' as aio_status,
                        NULL as aio_text,
                        jsonb_agg(
                            jsonb_build_object(
                                'url', sr.url,
                                'link', sr.url,
                                'title', sr.title,
                                'snippet', sr.snippet,
                                'position', sr.position,
                                'source', sr.source
                            ) ORDER BY sr.position
                        )::text as cited_sources_json,
                        MAX(sr.fetched_at) as snapshot_at,
                        'en-US' as locale,
                        'serp_results' as source_table
                    FROM serp_results sr
                    WHERE sr.query IS NOT NULL AND sr.query != ''
                    GROUP BY sr.query
                    ORDER BY snapshot_at DESC
                """
                
                # ê³ ìœ  ì¿¼ë¦¬ ìˆ˜ í™•ì¸
                cur.execute("SELECT COUNT(DISTINCT query) FROM serp_results WHERE query IS NOT NULL AND query != ''")
                unique_query_count = cur.fetchone()[0]
                print(f"serp_results í…Œì´ë¸” ê³ ìœ  ì¿¼ë¦¬ ìˆ˜: {unique_query_count}")
                
                cur.execute("SELECT COUNT(*) FROM serp_results")
                count_serp = cur.fetchone()[0]
                print(f"serp_results í…Œì´ë¸” ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {count_serp}")
                
                if count_serp > 0:
                    df_serp = pd.read_sql_query(query_serp, conn)
                    dfs.append(df_serp)
                    print(f"serp_resultsì—ì„œ {len(df_serp)}ê°œ ì¿¼ë¦¬ ì¡°íšŒ (ì˜ˆìƒ: {unique_query_count}ê°œ)")
        
        # ë°ì´í„°í”„ë ˆì„ í†µí•©
        if dfs:
            df = pd.concat(dfs, ignore_index=True)
            # ì¤‘ë³µ ì¿¼ë¦¬ ì œê±° (raw_serp_aio ìš°ì„ )
            # ë‹¨, serp_resultsì˜ ëª¨ë“  ì¿¼ë¦¬ë¥¼ í¬í•¨í•˜ë„ë¡ ì²˜ë¦¬
            before_dedup = len(df)
            df = df.drop_duplicates(subset=['query'], keep='first')
            after_dedup = len(df)
            print(f"get_serp_aio() í†µí•© ì „: {before_dedup}ê°œ, ì¤‘ë³µ ì œê±° í›„: {after_dedup}ê°œ")
            return df
        else:
            print("Warning: ë‘ í…Œì´ë¸” ëª¨ë‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"Error in get_serp_aio: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()
    finally:
        if conn:
            conn.close()

def get_clustering_results_from_db() -> pd.DataFrame:
    """DBì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì „ì²´ ì¡°íšŒ (Clustering Results íƒ­ìš©)"""
    conn = get_db_connection()
    if conn is None:
        print("âš ï¸ get_clustering_results_from_db: DB ì—°ê²° ì‹¤íŒ¨")
        return pd.DataFrame()
    
    try:
        query = """
            SELECT 
                c.cluster_id,
                c.size,
                c.algorithm,
                c.topic_category,
                c.sub_cluster_index,
                c.top_keywords,
                -- í´ëŸ¬ìŠ¤í„°ëª… ìƒì„±: topic_category_sub_cluster_index í˜•ì‹
                CASE 
                    WHEN c.topic_category IS NOT NULL AND c.sub_cluster_index IS NOT NULL 
                    THEN c.topic_category || '_' || (c.sub_cluster_index + 1)::text
                    ELSE 'Cluster_' || c.cluster_id::text
                END as cluster_name,
                -- ì „ì²´ í¬ìŠ¤íŠ¸ ID ëª©ë¡ (JSONBë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
                COALESCE(
                    jsonb_agg(DISTINCT ca.doc_id) FILTER (WHERE ca.doc_id IS NOT NULL)::text,
                    '[]'
                ) as post_ids,
                -- ëŒ€í‘œ í¬ìŠ¤íŠ¸ ID ëª©ë¡ (JSONBë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
                COALESCE(
                    jsonb_agg(DISTINCT ca.doc_id) FILTER (WHERE ca.is_representative = TRUE AND ca.doc_id IS NOT NULL)::text,
                    '[]'
                ) as representative_post_ids,
                COUNT(DISTINCT ca.doc_id) FILTER (WHERE ca.is_representative = TRUE) as representative_count
            FROM clusters c
            LEFT JOIN cluster_assignments ca ON c.cluster_id = ca.cluster_id
            WHERE c.noise_label = FALSE
            GROUP BY c.cluster_id, c.size, c.algorithm, c.topic_category, c.sub_cluster_index, c.top_keywords
            ORDER BY c.topic_category, c.sub_cluster_index, c.size DESC
        """
        df = pd.read_sql_query(query, conn)
        
        print(f"âœ… get_clustering_results_from_db: {len(df)}ê°œ í´ëŸ¬ìŠ¤í„° ì¡°íšŒë¨")
        
        # JSONB ë°°ì—´ì„ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if len(df) > 0:
            import json
            import numpy as np
            
            def safe_convert_to_list(val):
                """ê°’ì„ ì•ˆì „í•˜ê²Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
                if val is None:
                    return []
                if isinstance(val, list):
                    return val
                if isinstance(val, np.ndarray):
                    return val.tolist()
                if isinstance(val, str):
                    try:
                        parsed = json.loads(val)
                        return parsed if isinstance(parsed, list) else []
                    except (json.JSONDecodeError, TypeError):
                        return []
                try:
                    # dictë‚˜ ë‹¤ë¥¸ iterableì¸ ê²½ìš°
                    if hasattr(val, '__iter__') and not isinstance(val, (str, bytes)):
                        return list(val)
                except:
                    pass
                return []
            
            for idx, row in df.iterrows():
                try:
                    # post_ids ë³€í™˜
                    post_ids_val = row.get('post_ids')
                    df.at[idx, 'post_ids'] = safe_convert_to_list(post_ids_val)
                    
                    # representative_post_ids ë³€í™˜
                    rep_post_ids_val = row.get('representative_post_ids')
                    df.at[idx, 'representative_post_ids'] = safe_convert_to_list(rep_post_ids_val)
                    
                    # top_keywords ë³€í™˜
                    top_keywords_val = row.get('top_keywords')
                    df.at[idx, 'top_keywords'] = safe_convert_to_list(top_keywords_val)
                    
                except Exception as row_error:
                    print(f"âš ï¸ í–‰ {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {row_error}")
                    import traceback
                    traceback.print_exc()
                    # ê¸°ë³¸ê°’ ì„¤ì •
                    df.at[idx, 'post_ids'] = []
                    df.at[idx, 'representative_post_ids'] = []
                    df.at[idx, 'top_keywords'] = []
        
        return df
    except Exception as e:
        print(f"âŒ Error in get_clustering_results_from_db: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()
    finally:
        if conn:
            conn.close()

def get_clusters_with_trends() -> pd.DataFrame:
    """í´ëŸ¬ìŠ¤í„° ë° íŠ¸ë Œë“œ ì •ë³´ ì¡°íšŒ"""
    query = """
        SELECT 
            c.cluster_id,
            c.size,
            c.algorithm,
            c.topic_category,
            c.sub_cluster_index,
            c.top_keywords,
            c.monthly_trend_summary,
            c.representative_posts_summary,
            COUNT(DISTINCT ca.doc_id) FILTER (WHERE ca.is_representative = TRUE) as representative_count,
            (SELECT category FROM topic_qa_briefs WHERE cluster_id = c.cluster_id LIMIT 1) as category,
            -- í´ëŸ¬ìŠ¤í„°ëª… ìƒì„±: topic_category_sub_cluster_index í˜•ì‹ (ì˜ˆ: SPRING_RECIPES_1)
            CASE 
                WHEN c.topic_category IS NOT NULL AND c.sub_cluster_index IS NOT NULL 
                THEN c.topic_category || '_' || (c.sub_cluster_index + 1)::text
                ELSE 'Cluster_' || c.cluster_id::text
            END as cluster_name
        FROM clusters c
        LEFT JOIN cluster_assignments ca ON c.cluster_id = ca.cluster_id
        WHERE c.noise_label = FALSE
        GROUP BY c.cluster_id, c.size, c.algorithm, c.topic_category, c.sub_cluster_index, c.top_keywords, c.monthly_trend_summary, c.representative_posts_summary
        ORDER BY c.size DESC
    """
    return query_to_dataframe(query)

def get_cluster_summary_from_db(cluster_id: str) -> Optional[str]:
    """DBì—ì„œ í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì¡°íšŒ (cluster_id ë¬¸ìì—´ë¡œ ë§¤ì¹­)"""
    conn = get_db_connection()
    if conn is None:
        return None
    try:
        with conn.cursor() as cur:
            # cluster_idê°€ ë¬¸ìì—´ì¸ ê²½ìš° topic_categoryì™€ sub_cluster_indexë¡œ ë§¤ì¹­ ì‹œë„
            # ì˜ˆ: "SPRING_RECIPES_1" -> topic_category="SPRING_RECIPES", sub_cluster_index=1
            if isinstance(cluster_id, str) and '_' in cluster_id:
                parts = cluster_id.rsplit('_', 1)
                if len(parts) == 2:
                    topic_category = parts[0]
                    try:
                        sub_cluster_index = int(parts[1]) - 1  # JSONì€ 1ë¶€í„° ì‹œì‘, DBëŠ” 0ë¶€í„° ì‹œì‘í•  ìˆ˜ ìˆìŒ
                        cur.execute("""
                            SELECT summary
                            FROM clusters
                            WHERE topic_category = %s 
                            AND sub_cluster_index = %s
                            LIMIT 1
                        """, (topic_category, sub_cluster_index))
                        result = cur.fetchone()
                        if result and result[0]:
                            return result[0]
                    except ValueError:
                        pass
            
            # ì •ìˆ˜í˜• cluster_idë¡œ ì§ì ‘ ì¡°íšŒ ì‹œë„
            try:
                cluster_id_int = int(cluster_id) if isinstance(cluster_id, str) else cluster_id
                cur.execute("""
                    SELECT summary
                    FROM clusters
                    WHERE cluster_id = %s
                    LIMIT 1
                """, (cluster_id_int,))
                result = cur.fetchone()
                if result and result[0]:
                    return result[0]
            except (ValueError, TypeError):
                pass
            
            return None
    finally:
        conn.close()

def get_cluster_timeseries(cluster_id: int) -> pd.DataFrame:
    """í´ëŸ¬ìŠ¤í„° ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ"""
    # numpy.int64 íƒ€ì… ì˜¤ë¥˜ ìˆ˜ì •: cluster_idë¥¼ Python intë¡œ ë³€í™˜
    import numpy as np
    if isinstance(cluster_id, (np.integer, np.int64)):
        cluster_id = int(cluster_id)
    elif pd.api.types.is_integer(cluster_id):
        cluster_id = int(cluster_id)
    
    query = """
        SELECT 
            month,
            reddit_post_count,
            reddit_weighted_score
        FROM cluster_timeseries
        WHERE cluster_id = %s
        ORDER BY month DESC
    """
    return query_to_dataframe(query, (cluster_id,))

def get_cluster_representative_posts(cluster_id: int, limit: int = 5) -> pd.DataFrame:
    """í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ í¬ìŠ¤íŠ¸ ì¡°íšŒ"""
    # numpy.int64 íƒ€ì… ì˜¤ë¥˜ ìˆ˜ì •: cluster_idë¥¼ Python intë¡œ ë³€í™˜
    import numpy as np
    if isinstance(cluster_id, (np.integer, np.int64)):
        cluster_id = int(cluster_id)
    elif pd.api.types.is_integer(cluster_id):
        cluster_id = int(cluster_id)
    
    query = """
        SELECT 
            rp.reddit_post_id,
            rp.title,
            rp.body,
            rp.upvotes,
            rp.num_comments,
            rp.permalink,
            rp.keyword,
            TO_TIMESTAMP(rp.created_utc) as created_at
        FROM raw_reddit_posts rp
        JOIN cluster_assignments ca ON ca.doc_id = rp.reddit_post_id
        WHERE ca.cluster_id = %s
        AND ca.is_representative = TRUE
        ORDER BY rp.upvotes DESC
        LIMIT %s
    """
    return query_to_dataframe(query, (cluster_id, limit))

def get_cluster_gpt_summaries(cluster_id: int) -> Dict[str, Optional[str]]:
    """í´ëŸ¬ìŠ¤í„°ì˜ GPT ìš”ì•½ ì¡°íšŒ (ì›”ê°„ íŠ¸ë Œë“œ ë° ëŒ€í‘œ í¬ìŠ¤íŠ¸)"""
    conn = get_db_connection()
    if conn is None:
        return {"monthly_trend_summary": None, "representative_posts_summary": None}
    
    try:
        with conn.cursor() as cur:
            # numpy.int64 íƒ€ì… ì˜¤ë¥˜ ìˆ˜ì •: cluster_idë¥¼ Python intë¡œ ë³€í™˜
            import numpy as np
            if isinstance(cluster_id, (np.integer, np.int64)):
                cluster_id = int(cluster_id)
            elif pd.api.types.is_integer(cluster_id):
                cluster_id = int(cluster_id)
            
            cur.execute("""
                SELECT 
                    monthly_trend_summary,
                    representative_posts_summary
                FROM clusters
                WHERE cluster_id = %s
                LIMIT 1
            """, (cluster_id,))
            result = cur.fetchone()
            
            if result:
                return {
                    "monthly_trend_summary": result[0],
                    "representative_posts_summary": result[1]
                }
            else:
                return {"monthly_trend_summary": None, "representative_posts_summary": None}
    finally:
        conn.close()

def get_master_topics(category_filter: Optional[str] = None, 
                     trend_filter: Optional[str] = None,
                     aio_filter: Optional[str] = None,
                     lg_cited_filter: Optional[bool] = None) -> pd.DataFrame:
    """Master Topic ì¡°íšŒ (í•„í„° ì§€ì›)"""
    query = """
        SELECT 
            tqb.id,
            tqb.cluster_id,
            tqb.category,
            tqb.topic_title,
            tqb.primary_question,
            tqb.related_questions_json,
            tqb.score,
            tqb.insights_json->'evidence_strength'->>'score' as evidence_score,
            tqb.why_now_json,
            tqb.blog_angle,
            tqb.social_angle,
            tqb.evidence_pack_json,
            tqb.insights_json,
            c.size as cluster_size
        FROM topic_qa_briefs tqb
        JOIN clusters c ON tqb.cluster_id = c.cluster_id
        WHERE 1=1
    """
    params = []
    
    if category_filter:
        query += " AND tqb.category = %s"
        params.append(category_filter)
    
    query += " ORDER BY tqb.score DESC NULLS LAST"
    
    return query_to_dataframe(query, tuple(params) if params else None)

def get_serp_aio_audit() -> pd.DataFrame:
    """SERP AI Overview Audit ë°ì´í„° ì¡°íšŒ"""
    query = """
        SELECT 
            sa.query,
            sa.aio_status,
            sa.aio_text,
            sa.cited_sources_json,
            sa.snapshot_at,
            (SELECT topic_title FROM topic_qa_briefs 
             WHERE cluster_id IN (
                 SELECT DISTINCT ca.cluster_id 
                 FROM cluster_assignments ca
                 JOIN raw_reddit_posts rp ON ca.doc_id = rp.reddit_post_id
                 WHERE rp.keyword ILIKE '%' || sa.query || '%'
                 LIMIT 1
             ) LIMIT 1) as master_topic
        FROM raw_serp_aio sa
        ORDER BY sa.snapshot_at DESC
    """
    return query_to_dataframe(query)

def check_lg_domain(url: str) -> bool:
    """URLì´ LG ë„ë©”ì¸ì¸ì§€ í™•ì¸"""
    if not url:
        return False
    lg_domains = ['lge.com', 'lg.com', 'lgstory.com', 'lg.co.kr']
    url_lower = url.lower()
    return any(domain in url_lower for domain in lg_domains)

def parse_cited_sources(cited_sources_json: Any) -> List[Dict[str, Any]]:
    """Cited sources JSON íŒŒì‹± (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” {'sources': [...]} í˜•ì‹ ì§€ì›)"""
    if not cited_sources_json:
        return []
    
    if isinstance(cited_sources_json, str):
        import json
        try:
            cited_sources_json = json.loads(cited_sources_json)
        except:
            return []
    
    # {'sources': [...]} í˜•ì‹ ì²˜ë¦¬
    if isinstance(cited_sources_json, dict):
        if 'sources' in cited_sources_json:
            cited_sources_json = cited_sources_json['sources']
        else:
            # ë”•ì…”ë„ˆë¦¬ ìì²´ê°€ í•˜ë‚˜ì˜ ì†ŒìŠ¤ì¸ ê²½ìš°
            cited_sources_json = [cited_sources_json]
    
    if not isinstance(cited_sources_json, list):
        return []
    
    sources = []
    for source in cited_sources_json:
        if isinstance(source, dict):
            url = source.get('link') or source.get('url') or ''
            domain = ''
            if url:
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(url)
                    domain = parsed.netloc.replace('www.', '')
                except:
                    domain = url.split('/')[2] if len(url.split('/')) > 2 else url
            
            sources.append({
                'url': url,
                'domain': domain,
                'title': source.get('title', ''),
                'is_lg': check_lg_domain(url),
                'type': 'brand' if check_lg_domain(url) else ('media' if any(d in domain for d in ['cnn', 'bbc', 'nytimes']) else 'community')
            })
    
    return sources

def get_reddit_clustering_for_master_topic(topic_category: str) -> List[Dict[str, Any]]:
    """ë§ˆìŠ¤í„° í† í”½ ìƒì„±ì„ ìœ„í•œ Reddit í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì¡°íšŒ"""
    conn = get_db_connection()
    if conn is None:
        print("âš ï¸ get_reddit_clustering_for_master_topic: DB ì—°ê²° ì‹¤íŒ¨")
        return []
    
    try:
        with conn.cursor() as cur:
            # ë¨¼ì € í•´ë‹¹ topic_categoryì˜ í´ëŸ¬ìŠ¤í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            cur.execute("""
                SELECT COUNT(*) 
                FROM clusters 
                WHERE noise_label = FALSE 
                AND topic_category = %s
            """, (topic_category,))
            cluster_count = cur.fetchone()[0]
            print(f"ğŸ“Š {topic_category} ì¹´í…Œê³ ë¦¬ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜: {cluster_count}ê°œ")
            
            if cluster_count == 0:
                # topic_categoryê°€ NULLì¸ ê²½ìš°ë„ í™•ì¸
                cur.execute("""
                    SELECT COUNT(*) 
                    FROM clusters 
                    WHERE noise_label = FALSE 
                    AND topic_category IS NULL
                """)
                null_count = cur.fetchone()[0]
                print(f"â„¹ï¸ topic_categoryê°€ NULLì¸ í´ëŸ¬ìŠ¤í„° ìˆ˜: {null_count}ê°œ")
                
                # ì „ì²´ í´ëŸ¬ìŠ¤í„° ìˆ˜ í™•ì¸
                cur.execute("""
                    SELECT COUNT(*) 
                    FROM clusters 
                    WHERE noise_label = FALSE
                """)
                total_count = cur.fetchone()[0]
                print(f"â„¹ï¸ ì „ì²´ í´ëŸ¬ìŠ¤í„° ìˆ˜ (noise ì œì™¸): {total_count}ê°œ")
            
            query = """
                SELECT 
                    c.cluster_id,
                    c.topic_category,
                    c.sub_cluster_index,
                    c.size as cluster_size,
                    c.top_keywords,
                    -- ëŒ€í‘œ í¬ìŠ¤íŠ¸ ìš”ì•½ (ìƒìœ„ 3ê°œ í¬ìŠ¤íŠ¸ì˜ ì œëª©ê³¼ ë³¸ë¬¸ ì¼ë¶€)
                    COALESCE(
                        jsonb_agg(
                            jsonb_build_object(
                                'title', rp.title,
                                'body', LEFT(rp.body, 500),
                                'upvotes', rp.upvotes
                            ) ORDER BY rp.upvotes DESC
                        ) FILTER (WHERE ca.is_representative = TRUE AND ca.doc_type = 'reddit_post' AND rp.title IS NOT NULL),
                        '[]'::jsonb
                    ) as representative_posts
                FROM clusters c
                LEFT JOIN cluster_assignments ca ON c.cluster_id = ca.cluster_id AND ca.doc_type = 'reddit_post'
                LEFT JOIN raw_reddit_posts rp ON ca.doc_id = rp.reddit_post_id
                WHERE c.noise_label = FALSE
                AND c.topic_category = %s
                GROUP BY c.cluster_id, c.topic_category, c.sub_cluster_index, c.size, c.top_keywords
                ORDER BY c.sub_cluster_index, c.size DESC
            """
            cur.execute(query, (topic_category,))
            results = cur.fetchall()
            
            print(f"âœ… {topic_category} ì¹´í…Œê³ ë¦¬ í´ëŸ¬ìŠ¤í„° {len(results)}ê°œ ì¡°íšŒë¨")
            
            clusters = []
            for row in results:
                cluster_id, topic_cat, sub_idx, size, top_keywords, rep_posts = row
                
                # top_keywordsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                keywords_list = []
                if top_keywords:
                    import json
                    if isinstance(top_keywords, str):
                        try:
                            keywords_list = json.loads(top_keywords)
                        except:
                            keywords_list = []
                    elif isinstance(top_keywords, list):
                        keywords_list = top_keywords
                
                # representative_postsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                posts_list = []
                if rep_posts:
                    import json
                    if isinstance(rep_posts, str):
                        try:
                            posts_list = json.loads(rep_posts)
                        except:
                            posts_list = []
                    elif isinstance(rep_posts, list):
                        posts_list = rep_posts
                
                clusters.append({
                    'cluster_id': cluster_id,
                    'topic_category': topic_cat,
                    'sub_cluster_id': sub_idx,
                    'cluster_size': size,
                    'top_keywords': keywords_list[:20] if keywords_list else [],  # ìƒìœ„ 20ê°œë§Œ
                    'summary': None,  # DB summary ì œê±°, GPT ìš”ì•½ë§Œ ì‚¬ìš©
                    'representative_posts': posts_list[:3] if posts_list else []  # ìƒìœ„ 3ê°œë§Œ
                })
            
            return clusters
    except Exception as e:
        print(f"âŒ Error in get_reddit_clustering_for_master_topic: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        if conn:
            conn.close()

def get_serp_questions_for_master_topic(topic_category: str) -> List[str]:
    """ë§ˆìŠ¤í„° í† í”½ ìƒì„±ì„ ìœ„í•œ SERP ì§ˆë¬¸í˜• í‚¤ì›Œë“œ ì¡°íšŒ"""
    conn = get_db_connection()
    if conn is None:
        print("âš ï¸ get_serp_questions_for_master_topic: DB ì—°ê²° ì‹¤íŒ¨")
        return []
    
    try:
        with conn.cursor() as cur:
            queries = []
            
            # 1. serp_results í…Œì´ë¸” í™•ì¸ (topic_categoryê°€ ìˆëŠ” í…Œì´ë¸”)
            cur.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'serp_results'
                )
            """)
            serp_results_exists = cur.fetchone()[0]
            
            if serp_results_exists:
                # serp_resultsì— topic_category ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                cur.execute("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = 'serp_results' AND column_name = 'topic_category'
                """)
                has_topic_category_sr = cur.fetchone() is not None
                
                if has_topic_category_sr:
                    # topic_categoryë¡œ í•„í„°ë§
                    query = """
                        SELECT DISTINCT query
                        FROM serp_results
                        WHERE topic_category = %s
                        AND query IS NOT NULL
                        AND query != ''
                        ORDER BY query
                        LIMIT 100
                    """
                    cur.execute(query, (topic_category,))
                    serp_queries = [row[0] for row in cur.fetchall()]
                    print(f"âœ… serp_resultsì—ì„œ {len(serp_queries)}ê°œ ì¿¼ë¦¬ ì¡°íšŒ (topic_category={topic_category})")
                    queries.extend(serp_queries)
                else:
                    print("âš ï¸ serp_results í…Œì´ë¸”ì— topic_category ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # 2. raw_serp_aio í…Œì´ë¸” í™•ì¸ (topic_categoryê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
            cur.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'raw_serp_aio'
                )
            """)
            raw_serp_aio_exists = cur.fetchone()[0]
            
            if raw_serp_aio_exists:
                # raw_serp_aioì— topic_category ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                cur.execute("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = 'raw_serp_aio' AND column_name = 'topic_category'
                """)
                has_topic_category_aio = cur.fetchone() is not None
                
                if has_topic_category_aio:
                    # topic_categoryë¡œ í•„í„°ë§
                    query = """
                        SELECT DISTINCT query
                        FROM raw_serp_aio
                        WHERE topic_category = %s
                        AND query IS NOT NULL
                        AND query != ''
                        ORDER BY query
                        LIMIT 100
                    """
                    cur.execute(query, (topic_category,))
                    aio_queries = [row[0] for row in cur.fetchall()]
                    print(f"âœ… raw_serp_aioì—ì„œ {len(aio_queries)}ê°œ ì¿¼ë¦¬ ì¡°íšŒ (topic_category={topic_category})")
                    queries.extend(aio_queries)
                else:
                    # topic_categoryê°€ ì—†ìœ¼ë©´ ì „ì²´ ì¡°íšŒ (ë””ë²„ê¹…ìš©)
                    query = """
                        SELECT DISTINCT query
                        FROM raw_serp_aio
                        WHERE query IS NOT NULL
                        AND query != ''
                        ORDER BY query
                        LIMIT 200
                    """
                    cur.execute(query)
                    all_aio_queries = [row[0] for row in cur.fetchall()]
                    print(f"â„¹ï¸ raw_serp_aioì—ì„œ ì „ì²´ {len(all_aio_queries)}ê°œ ì¿¼ë¦¬ ì¡°íšŒ (topic_category ì»¬ëŸ¼ ì—†ìŒ)")
                    # ì¼ë‹¨ ì „ì²´ë¥¼ í¬í•¨ (ë‚˜ì¤‘ì— í•„í„°ë§ ê°€ëŠ¥)
                    queries.extend(all_aio_queries)
            
            # ì¤‘ë³µ ì œê±°
            unique_queries = list(set(queries))
            print(f"ğŸ“Š ì¤‘ë³µ ì œê±° ì „: {len(queries)}ê°œ, ì¤‘ë³µ ì œê±° í›„: {len(unique_queries)}ê°œ")
            
            # ì§ˆë¬¸í˜• í‚¤ì›Œë“œë§Œ í•„í„°ë§ (?, how, what, why, when, whereë¡œ ì‹œì‘í•˜ê±°ë‚˜ í¬í•¨)
            question_queries = [
                q for q in unique_queries 
                if any(q.lower().startswith(prefix) or '?' in q.lower() 
                       for prefix in ['how', 'what', 'why', 'when', 'where', 'which', 'who', 'can', 'should', 'is', 'are', 'do', 'does'])
            ]
            
            print(f"âœ… ì§ˆë¬¸í˜• í‚¤ì›Œë“œ í•„í„°ë§ í›„: {len(question_queries)}ê°œ")
            
            return question_queries[:100]  # ìµœëŒ€ 100ê°œ
    except Exception as e:
        print(f"âŒ Error in get_serp_questions_for_master_topic: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        if conn:
            conn.close()
