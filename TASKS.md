# TASKS.md - Kitchen Seasonal Content POC 구현 작업 목록

본 문서는 PoC 프로젝트의 구현 순서를 고정하기 위한 작업 체크리스트입니다.  
각 단계는 순차적으로 진행하며, 이전 단계가 완료되어야 다음 단계를 시작할 수 있습니다.

---

## 구현 단계 개요

1. [ ] Repository 구조 설계
2. [ ] 환경 변수 및 설정 전략 정리
3. [ ] Database 스키마 설계(DDL)
4. [ ] Reddit 수집 모듈 구현(Apify)
5. [ ] Google SERP AI Overview 수집 모듈(SerpAPI)
6. [ ] Google Search Console CSV 업로드 및 적재
7. [ ] 데이터 정제 및 중복 제거
8. [ ] 임베딩 생성 및 저장
9. [ ] HDBSCAN 클러스터링 실행
10. [ ] 클러스터 대표 샘플 및 특징어 추출
11. [ ] 클러스터 후처리 및 Intent 매핑(4대 주제)
12. [ ] LLM 기반 클러스터 해석(Q&A / 주제 / Why Now)
13. [ ] 월 단위 시계열 지표 생성
14. [ ] Streamlit 대시보드 구현
15. [ ] Railway 배포(web / worker)
16. [ ] PoC 검증 및 결과 정리
17. [ ] 리팩토링 및 최소 테스트

---

## 1. Repository 구조 설계

### 작업 목적
프로젝트의 폴더 구조와 모듈 구성을 명확히 정의하여, 이후 개발 작업의 일관성을 보장한다.

### 핵심 작업 항목
- [ ] `web/` 폴더 구조 설계 (Streamlit 앱 모듈)
- [ ] `worker/` 폴더 구조 설계 (데이터 수집/분석 파이프라인)
- [ ] `common/` 폴더 구조 설계 (DB, 설정, 유틸리티)
- [ ] `data/` 폴더 용도 정의 (임시 CSV, 중간 산출물)
- [ ] `tests/` 폴더 구조 설계 (최소 테스트 스크립트)
- [ ] `migrations/` 폴더 구조 설계 (DDL SQL 파일)
- [ ] 각 모듈별 `__init__.py` 파일 배치 계획
- [ ] `.gitignore` 규칙 최종 확인

### 완료 조건
- [ ] 모든 폴더 구조가 README.md에 문서화됨
- [ ] 각 폴더의 역할이 명확히 정의됨
- [ ] `.gitignore`가 `data/`, `.env`, `__pycache__/` 등을 포함함
- [ ] SPEC.md의 아키텍처와 일치함

---

## 2. 환경 변수 및 설정 전략 정리

### 작업 목적
프로젝트 전반에서 사용할 환경 변수와 설정 관리 방식을 정의한다.

### 핵심 작업 항목
- [ ] `.env.example` 파일 작성 (모든 환경 변수 템플릿)
- [ ] `common/config.py` 모듈 설계 (환경 변수 로드 로직)
- [ ] 데이터베이스 연결 정보 환경 변수 정의
- [ ] API 키 환경 변수 정의 (Reddit, SerpAPI, LLM)
- [ ] Railway 배포 환경 변수 정의
- [ ] 로컬 개발 환경 변수 정의
- [ ] 설정 검증 로직 설계 (필수 환경 변수 체크)

### 완료 조건
- [ ] `.env.example`에 모든 환경 변수가 주석과 함께 정의됨
- [ ] `common/config.py`가 환경 변수를 로드하고 검증함
- [ ] 필수 환경 변수 누락 시 명확한 에러 메시지 출력
- [ ] Railway와 로컬 환경 모두에서 동작함

---

## 3. Database 스키마 설계(DDL)

### 작업 목적
PostgreSQL 데이터베이스의 테이블 구조를 정의하고 DDL을 작성한다.

### 핵심 작업 항목
- [ ] Raw 데이터 테이블 설계
  - `reddit_posts` (포스트 원본)
  - `reddit_comments` (댓글 원본)
  - `serp_ai_overview` (SERP AIO 원본)
  - `gsc_data` (GSC CSV 원본)
- [ ] 정제 데이터 테이블 설계
  - `reddit_posts_cleaned` (정제된 포스트)
- [ ] 분석 결과 테이블 설계
  - `embeddings` (임베딩 벡터)
  - `clusters` (클러스터 메타데이터)
  - `cluster_keywords` (클러스터 특징어)
  - `cluster_samples` (대표 샘플)
- [ ] LLM 결과 테이블 설계
  - `llm_results` (Q&A brief, 주제, Why Now)
- [ ] 시계열 집계 테이블 설계
  - `monthly_trends_reddit` (Reddit 월별 집계)
  - `monthly_trends_gsc` (GSC 월별 집계)
- [ ] 인덱스 설계 (성능 최적화)
- [ ] DDL SQL 파일 작성 (`migrations/` 폴더)

### 완료 조건
- [ ] 모든 테이블 DDL이 `migrations/` 폴더에 SQL 파일로 작성됨
- [ ] 각 테이블의 컬럼 타입, 제약조건이 명확히 정의됨
- [ ] 인덱스가 쿼리 성능을 고려하여 설계됨
- [ ] DDL 실행 시 오류 없이 테이블 생성됨
- [ ] SPEC.md의 데이터 구조와 일치함

---

## 4. Reddit 수집 모듈 구현(Apify)

### 작업 목적
Apify를 통해 Reddit 데이터를 수집하고 데이터베이스에 저장한다.

### 핵심 작업 항목
- [ ] Apify API 클라이언트 설정
- [ ] 키워드 리스트 정의 (4개 대주제 기준)
- [ ] Apify Actor 선택 및 파라미터 설정
  - 키워드별 최대 1,000개 포스트
  - 업보트/댓글 수 기준 정렬
  - 댓글 Top 3 수집
- [ ] 수집 스크립트 작성 (`worker/collectors/reddit_collector.py`)
- [ ] 데이터 파싱 및 정규화 로직
- [ ] 데이터베이스 저장 로직 (`reddit_posts`, `reddit_comments` 테이블)
- [ ] 에러 핸들링 및 재시도 로직
- [ ] 수집 진행 상황 로깅

### 완료 조건
- [ ] 키워드별 최소 1개 포스트 수집 성공
- [ ] 수집된 데이터가 `reddit_posts` 테이블에 저장됨
- [ ] 댓글 Top 3가 `reddit_comments` 테이블에 저장됨
- [ ] 수집 실패 시 명확한 에러 메시지 출력
- [ ] 수집 로그가 파일 또는 콘솔에 기록됨

### 제외 범위
- 실시간 수집 (배치만 지원)
- Reddit API 직접 호출 (Apify 사용)

---

## 5. Google SERP AI Overview 수집 모듈(SerpAPI)

### 작업 목적
SerpAPI를 통해 Google SERP AI Overview 데이터를 1회 스냅샷으로 수집한다.

### 핵심 작업 항목
- [ ] SerpAPI 클라이언트 설정
- [ ] 수집 대상 키워드 리스트 정의 (주요 키워드 10-20개)
- [ ] SerpAPI 호출 스크립트 작성 (`worker/collectors/serp_collector.py`)
- [ ] AI Overview 존재 여부 확인 로직
- [ ] AI Overview 텍스트 추출 로직
- [ ] 질문 리스트 추출 로직
- [ ] 데이터베이스 저장 로직 (`serp_ai_overview` 테이블)
- [ ] 에러 핸들링 (AI Overview 없음 처리)

### 완료 조건
- [ ] 최소 1개 키워드에 대해 SerpAPI 호출 성공
- [ ] AI Overview가 있는 경우 데이터 추출 및 저장 성공
- [ ] AI Overview가 없는 경우에도 에러 없이 처리됨
- [ ] 수집된 데이터가 `serp_ai_overview` 테이블에 저장됨
- [ ] 수집 로그가 기록됨

### 제외 범위
- 실시간 수집 (1회 스냅샷만)
- 기간 필터링 (스냅샷이므로 기간 무관)

---

## 6. Google Search Console CSV 업로드 및 적재

### 작업 목적
GSC CSV 파일을 업로드하고 파싱하여 데이터베이스에 저장한다.

### 핵심 작업 항목
- [ ] CSV 파일 업로드 인터페이스 설계 (Streamlit 또는 CLI)
- [ ] CSV 스키마 검증 로직 (필수 컬럼: Query, Date, Impressions, Clicks, CTR, Position)
- [ ] CSV 파싱 스크립트 작성 (`worker/collectors/gsc_loader.py`)
- [ ] 날짜 범위 검증 (작년 1월 1일 ~ 12월 31일)
- [ ] 데이터 타입 변환 및 정규화
- [ ] 데이터베이스 저장 로직 (`gsc_data` 테이블)
- [ ] 중복 데이터 처리 (재업로드 시)
- [ ] 업로드 결과 피드백 (성공/실패, 행 수)

### 완료 조건
- [ ] CSV 파일 업로드 및 파싱 성공
- [ ] 필수 컬럼 검증 통과
- [ ] 날짜 범위 검증 통과
- [ ] 수집된 데이터가 `gsc_data` 테이블에 저장됨
- [ ] 업로드 결과가 사용자에게 표시됨

### 제외 범위
- GSC API 직접 연동 (CSV 업로드만 지원)
- 실시간 데이터 동기화

---

## 7. 데이터 정제 및 중복 제거

### 작업 목적
수집된 Raw 데이터를 정제하고 중복을 제거하여 분석 가능한 상태로 만든다.

### 핵심 작업 항목
- [ ] 질문형/How-to/아이디어성 콘텐츠 필터링 로직 구현
- [ ] 언어 필터링 로직 (영어만 포함)
- [ ] 길이 필터링 로직 (최소/최대 길이)
- [ ] 노이즈 처리 로직 (HTML 태그 제거, 특수 문자 정규화)
- [ ] 중복 제거 로직
  - 제목 유사도 (Levenshtein 거리 80% 이상)
  - 본문 유사도 (임베딩 코사인 유사도 0.95 이상)
  - URL 중복
- [ ] 정제된 데이터 저장 (`reddit_posts_cleaned` 테이블)
- [ ] 정제 통계 로깅 (제거된 행 수, 남은 행 수)

### 완료 조건
- [ ] 질문형/How-to/아이디어성 콘텐츠만 남음
- [ ] 영어 콘텐츠만 남음
- [ ] 중복 제거가 효과적으로 작동함 (중복률 10% 이하)
- [ ] 정제된 데이터가 `reddit_posts_cleaned` 테이블에 저장됨
- [ ] 정제 통계가 로그에 기록됨

---

## 8. 임베딩 생성 및 저장

### 작업 목적
정제된 Reddit 포스트에 대해 임베딩을 생성하고 데이터베이스에 저장한다.

### 핵심 작업 항목
- [ ] 임베딩 모델 선택 및 로드 (sentence-transformers)
- [ ] 텍스트 전처리 로직 (제목 + 본문 결합, 토큰 길이 제한)
- [ ] 임베딩 생성 스크립트 작성 (`worker/analyzers/embedding_generator.py`)
- [ ] 배치 처리 로직 (대량 데이터 처리)
- [ ] 임베딩 벡터 저장 로직 (`embeddings` 테이블)
- [ ] 진행 상황 로깅 (진행률 표시)
- [ ] 재실행 시 중복 생성 방지 (이미 임베딩이 있으면 스킵)

### 완료 조건
- [ ] 모든 정제된 포스트에 대해 임베딩 생성 성공
- [ ] 임베딩 벡터가 `embeddings` 테이블에 저장됨
- [ ] 임베딩 차원이 일관됨 (모델 출력 차원)
- [ ] 재실행 시 기존 임베딩을 재사용함
- [ ] 임베딩 생성 시간이 로그에 기록됨

### 제외 범위
- 벡터 DB 사용 (Postgres에 직접 저장)
- 댓글 임베딩 생성 (포스트만)

---

## 9. HDBSCAN 클러스터링 실행

### 작업 목적
임베딩 벡터를 기반으로 HDBSCAN 클러스터링을 실행하여 유사한 포스트를 그룹화한다.

### 핵심 작업 항목
- [ ] HDBSCAN 라이브러리 설정
- [ ] HDBSCAN 파라미터 튜닝
  - `min_cluster_size`: 5
  - `min_samples`: 3
  - `metric`: 'euclidean' 또는 'cosine'
- [ ] 클러스터링 실행 스크립트 작성 (`worker/analyzers/clusterer.py`)
- [ ] Noise 포인트 처리 로직 (별도 저장)
- [ ] 클러스터 메타데이터 저장 (`clusters` 테이블)
- [ ] 클러스터링 통계 로깅
  - 총 클러스터 수
  - Noise 포인트 수 및 비율
  - 평균/최대/최소 클러스터 크기

### 완료 조건
- [ ] 클러스터링 실행 성공 (에러 없음)
- [ ] 클러스터 메타데이터가 `clusters` 테이블에 저장됨
- [ ] Noise 포인트가 별도로 식별됨
- [ ] 클러스터링 통계가 로그에 기록됨
- [ ] 총 클러스터 수가 10개 이상 생성됨

---

## 10. 클러스터 대표 샘플 및 특징어 추출

### 작업 목적
각 클러스터의 대표 샘플과 특징어를 추출하여 LLM 입력 준비를 한다.

### 핵심 작업 항목
- [ ] Centroid 계산 로직 (클러스터 내 모든 임베딩 평균)
- [ ] 대표 샘플 선정 로직 (centroid와 가장 유사한 포스트)
- [ ] 대체 규칙 구현 (대표 샘플 없을 경우 업보트 수 기준)
- [ ] TF-IDF 기반 특징어 추출 로직
- [ ] 특징어 상위 10개 선정
- [ ] 대표 샘플 저장 (`cluster_samples` 테이블)
- [ ] 특징어 저장 (`cluster_keywords` 테이블)

### 완료 조건
- [ ] 모든 클러스터에 대해 대표 샘플이 선정됨
- [ ] 모든 클러스터에 대해 특징어 10개가 추출됨
- [ ] 대표 샘플이 `cluster_samples` 테이블에 저장됨
- [ ] 특징어가 `cluster_keywords` 테이블에 저장됨
- [ ] 대표 샘플이 클러스터를 잘 대표함 (수동 검증 가능)

---

## 11. 클러스터 후처리 및 Intent 매핑(4대 주제)

### 작업 목적
클러스터를 4개 대주제 카테고리에 매핑하고, 우선순위를 결정한다.

### 핵심 작업 항목
- [ ] Intent Taxonomy 정의 확인 (4개 대주제)
- [ ] 규칙 기반 매핑 로직 구현 (키워드 매칭)
- [ ] LLM 기반 매핑 로직 구현 (명확하지 않은 경우)
- [ ] 매핑 결과 저장 (`clusters` 테이블의 `category` 컬럼)
- [ ] 우선순위 계산 로직
  - 클러스터 크기
  - 평균 업보트
  - 최근성
  - GSC 연관성
- [ ] 상위 50개 클러스터 선정

### 완료 조건
- [ ] 모든 클러스터가 4개 대주제 중 하나에 매핑됨
- [ ] 매핑 결과가 `clusters` 테이블에 저장됨
- [ ] 우선순위가 계산되어 저장됨
- [ ] 상위 50개 클러스터가 선정됨
- [ ] 매핑 정확도가 90% 이상 (수동 검증)

---

## 12. LLM 기반 클러스터 해석(Q&A / 주제 / Why Now)

### 작업 목적
클러스터 단위로 LLM을 호출하여 Q&A brief, 주제, Why Now를 생성한다.

### 핵심 작업 항목
- [ ] LLM API 클라이언트 설정 (OpenAI, Anthropic 등)
- [ ] LLM 입력 구성 로직
  - 대표 샘플
  - 특징어
  - 월별 트렌드 요약
  - GSC 데이터 요약
  - SERP AIO 요약
- [ ] LLM 프롬프트 템플릿 작성
- [ ] JSON 스키마 정의 및 강제
- [ ] LLM 호출 스크립트 작성 (`worker/analyzers/llm_interpreter.py`)
- [ ] JSON 파싱 및 검증 로직
- [ ] 재시도 로직 (파싱 실패 시 최대 3회)
- [ ] 결과 저장 (`llm_results` 테이블)
- [ ] 캐시 로직 구현 (동일 클러스터 재호출 방지)
- [ ] 비용 모니터링 (호출 횟수, 토큰 사용량)

### 완료 조건
- [ ] 상위 50개 클러스터에 대해 LLM 호출 성공
- [ ] JSON 파싱 성공률이 95% 이상
- [ ] 모든 필수 필드가 생성됨 (category, topic_title, primary_question 등)
- [ ] 결과가 `llm_results` 테이블에 저장됨
- [ ] 캐시가 정상 작동함 (재호출 방지)
- [ ] 비용이 로그에 기록됨

### 제외 범위
- 문서 단위 LLM 호출 (클러스터 단위만)
- 자동화된 콘텐츠 생성 (주제 발굴까지만)

---

## 13. 월 단위 시계열 지표 생성

### 작업 목적
Reddit과 GSC 데이터를 월 단위로 집계하여 시계열 트렌드를 생성한다.

### 핵심 작업 항목
- [ ] Reddit 월별 집계 로직
  - `created_utc`를 YYYY-MM 형식으로 변환
  - 월별 포스트 수, 평균 업보트, 평균 댓글 수 계산
- [ ] GSC 월별 집계 로직
  - Date를 YYYY-MM 형식으로 변환
  - Query별 월별 Impressions, Clicks, CTR 계산
- [ ] 트렌드 계산 로직
  - 전월 대비 증감률
  - 3개월 이동 평균
- [ ] 트렌드 상태 판단 로직 (Emerging/Competitive/Saturated/Niche)
- [ ] 집계 결과 저장
  - `monthly_trends_reddit` 테이블
  - `monthly_trends_gsc` 테이블
- [ ] 클러스터-월별 트렌드 연결

### 완료 조건
- [ ] Reddit 월별 집계가 `monthly_trends_reddit` 테이블에 저장됨
- [ ] GSC 월별 집계가 `monthly_trends_gsc` 테이블에 저장됨
- [ ] 트렌드 상태가 계산되어 저장됨
- [ ] 클러스터와 월별 트렌드가 연결됨
- [ ] 집계 데이터가 시각화 가능한 형태로 준비됨

---

## 14. Streamlit 대시보드 구현

### 작업 목적
분석 결과를 시각화하고 콘텐츠 주제를 발굴할 수 있는 Streamlit 대시보드를 구현한다.

### 14.1 Raw 데이터 확인 화면

#### 핵심 작업 항목
- [ ] Reddit 포스트 목록 표시 (필터링 가능)
- [ ] GSC 데이터 표시 (Query별 Impressions, Clicks)
- [ ] SERP AIO 데이터 표시
- [ ] 데이터 품질 지표 표시 (중복률, 노이즈 비율)
- [ ] 필터링 옵션 구현
  - 키워드별 필터
  - 날짜 범위 필터
  - 서브레딧 필터
  - 업보트/댓글 수 기준 필터

#### 완료 조건
- [ ] 모든 Raw 데이터가 화면에 표시됨
- [ ] 필터링이 정상 작동함
- [ ] 데이터 품질 지표가 표시됨

### 14.2 클러스터 + 시계열 분석 화면

#### 핵심 작업 항목
- [ ] 클러스터 목록 표시 (카테고리별 그룹화)
- [ ] 클러스터 크기, 평균 업보트 표시
- [ ] 대표 샘플 미리보기
- [ ] 클러스터별 특징어 표시
- [ ] 시계열 차트 구현
  - 월별 포스트 수 추이
  - 월별 평균 업보트 추이
  - GSC Impressions 추이
- [ ] 트렌드 상태 표시 (Emerging/Competitive/Saturated/Niche)
- [ ] 인터랙션 기능 (클러스터 클릭 시 상세 정보)

#### 완료 조건
- [ ] 클러스터 목록이 카테고리별로 표시됨
- [ ] 시계열 차트가 정상 렌더링됨
- [ ] 트렌드 상태가 색상 또는 라벨로 표시됨
- [ ] 클러스터 클릭 시 상세 정보가 표시됨

### 14.3 콘텐츠 주제 발굴 화면

#### 핵심 작업 항목
- [ ] 주제 카드 표시 (카테고리별 탭)
- [ ] 각 주제 카드에 다음 정보 표시:
  - topic_title
  - primary_question
  - related_questions
  - blog_angle
  - social_angle
  - why_now
  - evidence_summary
- [ ] Evidence Pack 표시
  - Reddit 포스트 샘플
  - GSC 검색량 데이터
  - SERP AIO 존재 여부
  - 월별 트렌드 차트
- [ ] 필터링 및 정렬 기능
  - 카테고리별 필터
  - 트렌드 상태별 필터
  - 우선순위 정렬
- [ ] 내보내기 기능 (CSV 다운로드)

#### 완료 조건
- [ ] 주제 카드가 카테고리별로 표시됨
- [ ] 모든 필수 정보가 주제 카드에 표시됨
- [ ] Evidence Pack이 표시됨
- [ ] 필터링 및 정렬이 정상 작동함
- [ ] CSV 다운로드가 정상 작동함

### 전체 완료 조건
- [ ] 3개 화면이 모두 구현됨
- [ ] 화면 간 네비게이션이 작동함
- [ ] 데이터 로딩 시간이 3초 이하
- [ ] UI가 한국어로 제공됨

---

## 15. Railway 배포(web / worker)

### 작업 목적
Streamlit 대시보드와 Worker 파이프라인을 Railway에 배포한다.

### 15.1 Web 서비스 배포

#### 핵심 작업 항목
- [ ] Railway 프로젝트에 Streamlit 서비스 추가
- [ ] GitHub 저장소 연결 확인
- [ ] 환경 변수 설정 (DATABASE_URL, API 키 등)
- [ ] Dockerfile 빌드 확인
- [ ] 배포 후 접속 URL 확인
- [ ] 대시보드 정상 작동 확인

#### 완료 조건
- [ ] Streamlit 서비스가 Railway에 배포됨
- [ ] 배포 URL로 접속 시 대시보드가 표시됨
- [ ] 데이터베이스 연결이 정상 작동함
- [ ] 모든 화면이 정상 렌더링됨

### 15.2 Worker 서비스 배포

#### 핵심 작업 항목
- [ ] Railway 프로젝트에 Worker 서비스 추가
- [ ] Worker Dockerfile 설정 확인
- [ ] 환경 변수 설정 (DATABASE_URL, API 키 등)
- [ ] Worker 실행 명령 설정
- [ ] 배포 후 로그 확인
- [ ] 수동 실행 테스트

#### 완료 조건
- [ ] Worker 서비스가 Railway에 배포됨
- [ ] Worker가 정상 실행됨
- [ ] 데이터 수집이 정상 작동함
- [ ] 로그가 Railway 대시보드에 표시됨

### 전체 완료 조건
- [ ] Web과 Worker 서비스가 모두 배포됨
- [ ] 두 서비스가 동일한 데이터베이스에 연결됨
- [ ] 배포 상태가 SUCCESS임
- [ ] 재배포 시 자동으로 업데이트됨

---

## 16. PoC 검증 및 결과 정리

### 작업 목적
PoC의 성공 여부를 검증하고 결과를 정리한다.

### 핵심 작업 항목
- [ ] 정량 기준 검증
  - Reddit 데이터 수집량 (키워드별 최소 500개)
  - 클러스터 수 (50개 이상)
  - LLM 결과 수 (50개 이상)
  - JSON 파싱 성공률 (95% 이상)
- [ ] 정성 기준 검증
  - 주제 제목의 명확성
  - primary_question의 실용성
  - blog_angle과 social_angle의 실행 가능성
  - why_now의 설득력
  - Evidence Pack의 충분성
- [ ] 운영 기준 검증
  - 전체 파이프라인 재실행 성공
  - 배포 상태 안정성
  - 대시보드 로딩 시간 (3초 이하)
- [ ] 결과 문서 작성
  - 검증 결과 요약
  - 발견된 이슈 및 개선 사항
  - 다음 단계 제안

### 완료 조건
- [ ] 모든 정량 기준을 만족함
- [ ] 정성 기준이 충족됨 (수동 검증)
- [ ] 운영 기준이 충족됨
- [ ] 결과 문서가 작성됨
- [ ] SPEC.md의 성공 기준과 비교하여 평가됨

---

## 17. 리팩토링 및 최소 테스트

### 작업 목적
코드 품질을 개선하고 최소한의 테스트를 작성한다.

### 핵심 작업 항목
- [ ] 코드 리팩토링
  - 중복 코드 제거
  - 함수 분리 및 모듈화
  - 에러 핸들링 개선
  - 로깅 개선
- [ ] 최소 테스트 작성
  - 데이터 정제 로직 테스트
  - 임베딩 생성 테스트
  - 클러스터링 테스트
  - LLM 호출 모킹 테스트
- [ ] 문서화
  - 주요 함수 docstring 작성
  - README.md 업데이트
  - 실행 가이드 작성

### 완료 조건
- [ ] 주요 모듈이 리팩토링됨
- [ ] 최소 5개 이상의 테스트가 작성됨
- [ ] 테스트 실행 시 모든 테스트 통과
- [ ] 주요 함수에 docstring이 작성됨
- [ ] README.md가 업데이트됨

### 제외 범위
- 전체 테스트 커버리지 100% (최소 테스트만)
- E2E 테스트 (단위 테스트 중심)

---

## 작업 진행 체크리스트

각 단계를 완료할 때마다 아래 체크리스트를 업데이트하세요.

- [ ] 1. Repository 구조 설계
- [ ] 2. 환경 변수 및 설정 전략 정리
- [ ] 3. Database 스키마 설계(DDL)
- [ ] 4. Reddit 수집 모듈 구현(Apify)
- [ ] 5. Google SERP AI Overview 수집 모듈(SerpAPI)
- [ ] 6. Google Search Console CSV 업로드 및 적재
- [ ] 7. 데이터 정제 및 중복 제거
- [ ] 8. 임베딩 생성 및 저장
- [ ] 9. HDBSCAN 클러스터링 실행
- [ ] 10. 클러스터 대표 샘플 및 특징어 추출
- [ ] 11. 클러스터 후처리 및 Intent 매핑(4대 주제)
- [ ] 12. LLM 기반 클러스터 해석(Q&A / 주제 / Why Now)
- [ ] 13. 월 단위 시계열 지표 생성
- [ ] 14. Streamlit 대시보드 구현
- [ ] 15. Railway 배포(web / worker)
- [ ] 16. PoC 검증 및 결과 정리
- [ ] 17. 리팩토링 및 최소 테스트

---

## 주의사항

- 각 단계는 순차적으로 진행해야 하며, 이전 단계가 완료되어야 다음 단계를 시작할 수 있습니다.
- PoC 범위를 벗어나는 작업은 명시적으로 제외되어 있습니다.
- 완료 조건을 모두 만족해야 해당 단계를 완료한 것으로 간주합니다.
- 문제 발생 시 SPEC.md를 참고하여 설계 의도와 일치하는지 확인하세요.

---

**문서 버전**: 1.0  
**최종 수정일**: 2025-01-08  
**작성자**: Kitchen Seasonal Content POC Team
